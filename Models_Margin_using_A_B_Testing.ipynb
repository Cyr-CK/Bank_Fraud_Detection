{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "H6rZI4XSOf8g"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd  # For data manipulation and analysis\n",
        "import xgboost as xgb  # For using the XGBoost algorithm\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix  # For evaluating model performance\n",
        "\n",
        "# Load the preprocessed training and testing datasets from Parquet files\n",
        "# Parquet is a columnar storage file format optimized for use with big data processing frameworks\n",
        "train_df = pd.read_parquet(\"preprocessed_train_data.parquet\")  # Load the training dataset\n",
        "test_df = pd.read_parquet(\"preprocessed_test_data.parquet\")  # Load the testing dataset\n",
        "\n",
        "# Define features (independent variables) and target (dependent variable)\n",
        "# Features are all columns except the target column \"FlagImpaye\"\n",
        "X_train = train_df.drop(columns=[\"FlagImpaye\"])  # Features for the training set\n",
        "y_train = train_df[\"FlagImpaye\"].astype(int)  # Target for the training set, converted to numeric (int)\n",
        "\n",
        "X_test = test_df.drop(columns=[\"FlagImpaye\"])  # Features for the testing set\n",
        "y_test = test_df[\"FlagImpaye\"].astype(int)  # Target for the testing set, converted to numeric (int)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Cost-Sensitive Learning with XGBoost\n",
        "XGBoost provides the scale_pos_weight parameter to handle imbalanced datasets. This parameter adjusts the weight of the positive class (fraudulent transactions) relative to the negative class (non-fraudulent transactions)."
      ],
      "metadata": {
        "id": "IhBoxMOshW-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate scale_pos_weight for cost-sensitive learning\n",
        "# The scale_pos_weight parameter is used to handle class imbalance in binary classification.\n",
        "# It adjusts the weight of the positive class (fraudulent transactions) relative to the negative class (non-fraudulent transactions).\n",
        "# This is calculated as the ratio of the number of negative samples to the number of positive samples.\n",
        "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
        "\n",
        "# Train XGBoost model with cost-sensitive learning\n",
        "# Initialize the XGBoost classifier with specific parameters:\n",
        "# - scale_pos_weight: Adjusts the balance between classes based on the calculated ratio.\n",
        "# - objective=\"binary:logistic\": Specifies that this is a binary classification problem with logistic loss.\n",
        "# - eval_metric=\"logloss\": Sets the evaluation metric to log loss, which is commonly used for binary classification.\n",
        "# - random_state=42: Ensures reproducibility by fixing the random seed.\n",
        "model_xgb = xgb.XGBClassifier(\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    objective=\"binary:logistic\",\n",
        "    eval_metric=\"logloss\",\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit the XGBoost model to the training data\n",
        "# This trains the model using the features (X_train) and target variable (y_train).\n",
        "model_xgb.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "# Use the trained model to make predictions on the test set (X_test).\n",
        "y_pred = model_xgb.predict(X_test)\n",
        "\n",
        "# Print a classification report to evaluate model performance\n",
        "# The classification report includes metrics such as precision, recall, F1-score, and support for each class.\n",
        "# These metrics help assess how well the model performs, especially for imbalanced datasets.\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkcvUWo9Pd0Q",
        "outputId": "55f7e21c-26b4-41aa-ef94-36f282fe2511"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.68      0.81    740838\n",
            "           1       0.02      0.67      0.04      6573\n",
            "\n",
            "    accuracy                           0.68    747411\n",
            "   macro avg       0.51      0.67      0.42    747411\n",
            "weighted avg       0.99      0.68      0.80    747411\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Cost-Sensitive Learning with LightGBM\n",
        "LightGBM allows us to specify the is_unbalance parameter or provide instance weights directly."
      ],
      "metadata": {
        "id": "go6fbPQlhMzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "# Create LightGBM datasets\n",
        "# LightGBM uses its own Dataset format for training and evaluation.\n",
        "# `X_train` contains the feature matrix for the training set, and `y_train` contains the corresponding labels.\n",
        "train_data = lgb.Dataset(X_train, label=y_train)\n",
        "\n",
        "# Similarly, create a dataset for the test set. The `reference` parameter ensures that the test set aligns with the training set's structure.\n",
        "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
        "\n",
        "# Define parameters for cost-sensitive learning\n",
        "# These parameters configure the LightGBM model for binary classification with cost-sensitive adjustments.\n",
        "params = {\n",
        "    \"objective\": \"binary\",  # Specifies that this is a binary classification problem.\n",
        "    \"metric\": \"binary_logloss\",  # Use binary log loss as the evaluation metric.\n",
        "    \"is_unbalance\": True,  # Automatically adjusts for class imbalance by weighting classes inversely proportional to their frequency.\n",
        "    \"boosting_type\": \"gbdt\",  # Gradient Boosting Decision Tree (GBDT) is the default boosting method.\n",
        "    \"random_state\": 42  # Set a random seed for reproducibility.\n",
        "}\n",
        "\n",
        "# Train LightGBM model\n",
        "# The `train` function trains the model using the specified parameters and training dataset.\n",
        "model_lgb = lgb.train(params, train_data)\n",
        "\n",
        "# Evaluate the model\n",
        "# Predict probabilities for the test set. LightGBM outputs probabilities for the positive class by default.\n",
        "y_pred = model_lgb.predict(X_test)\n",
        "\n",
        "# Convert predicted probabilities into binary predictions (0 or 1).\n",
        "# A threshold of 0.5 is used: if the predicted probability is greater than 0.5, classify as 1; otherwise, classify as 0.\n",
        "y_pred_binary = [1 if x > 0.5 else 0 for x in y_pred]\n",
        "\n",
        "# Print a classification report to evaluate the model's performance.\n",
        "# This includes metrics like precision, recall, F1-score, and support for each class.\n",
        "print(classification_report(y_test, y_pred_binary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07g1YYKnPfmY",
        "outputId": "11fd245f-76c7-4e16-df18-313f9626eadf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 23346, number of negative: 3865122\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.104900 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2107\n",
            "[LightGBM] [Info] Number of data points in the train set: 3888468, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006004 -> initscore=-5.109323\n",
            "[LightGBM] [Info] Start training from score -5.109323\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.60      0.75    740838\n",
            "           1       0.01      0.66      0.03      6573\n",
            "\n",
            "    accuracy                           0.60    747411\n",
            "   macro avg       0.50      0.63      0.39    747411\n",
            "weighted avg       0.99      0.60      0.74    747411\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cost-Sensitive Learning with XGBoost + Bayesian Optimization\n",
        "Bayesian optimization can be used to fine-tune hyperparameters, including scale_pos_weight. We'll use the hyperopt library for this purpose."
      ],
      "metadata": {
        "id": "MPu6yN12g_-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hyperopt import fmin, tpe, hp, Trials\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Define the objective function for Bayesian optimization\n",
        "def objective(params):\n",
        "    \"\"\"\n",
        "    This function defines the objective to minimize during Bayesian optimization.\n",
        "    It trains an XGBoost model using the given hyperparameters and evaluates its performance\n",
        "    on the test set using the ROC-AUC score. The negative AUC is returned because `fmin`\n",
        "    minimizes the objective function.\n",
        "    \"\"\"\n",
        "    # Create an XGBoost classifier with the current set of hyperparameters\n",
        "    model = xgb.XGBClassifier(\n",
        "        scale_pos_weight=params['scale_pos_weight'],  # Weight for positive class (cost-sensitive learning)\n",
        "        max_depth=int(params['max_depth']),          # Maximum depth of the tree\n",
        "        learning_rate=params['learning_rate'],       # Step size shrinkage used in updates\n",
        "        n_estimators=int(params['n_estimators']),    # Number of boosting rounds\n",
        "        random_state=42                              # Fixed random seed for reproducibility\n",
        "    )\n",
        "\n",
        "    # Train the model on the training data\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict probabilities for the positive class on the test set\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Calculate the ROC-AUC score and return its negative value (to minimize)\n",
        "    return -roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "# Define the search space for hyperparameters\n",
        "space = {\n",
        "    'scale_pos_weight': hp.uniform('scale_pos_weight', 1, 10),  # Uniform distribution for scale_pos_weight\n",
        "    'max_depth': hp.quniform('max_depth', 3, 10, 1),           # Quantized uniform distribution for max_depth\n",
        "    'learning_rate': hp.loguniform('learning_rate', -5, 0),   # Log-uniform distribution for learning_rate\n",
        "    'n_estimators': hp.quniform('n_estimators', 50, 500, 50)   # Quantized uniform distribution for n_estimators\n",
        "}\n",
        "\n",
        "# Perform Bayesian optimization\n",
        "trials = Trials()  # Object to store details of each evaluation\n",
        "best_params = fmin(\n",
        "    fn=objective,            # Objective function to minimize\n",
        "    space=space,             # Search space for hyperparameters\n",
        "    algo=tpe.suggest,        # Tree-structured Parzen Estimator (TPE) algorithm for optimization\n",
        "    max_evals=20,            # Maximum number of evaluations\n",
        "    trials=trials            # Store trial information\n",
        ")\n",
        "\n",
        "# Train the final model with the best parameters found by Bayesian optimization\n",
        "best_model = xgb.XGBClassifier(\n",
        "    scale_pos_weight=best_params['scale_pos_weight'],  # Use the best scale_pos_weight\n",
        "    max_depth=int(best_params['max_depth']),          # Convert max_depth to integer\n",
        "    learning_rate=best_params['learning_rate'],       # Use the best learning_rate\n",
        "    n_estimators=int(best_params['n_estimators']),    # Convert n_estimators to integer\n",
        "    random_state=42                                    # Fixed random seed for reproducibility\n",
        ")\n",
        "best_model.fit(X_train, y_train)  # Train the model on the full training set\n",
        "\n",
        "# Evaluate the final model on the test set\n",
        "y_pred = best_model.predict(X_test)  # Predict class labels for the test set\n",
        "print(classification_report(y_test, y_pred))  # Print a detailed classification report"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXsJRv4UPia6",
        "outputId": "2e0d828f-3fcf-4c43-a850-e27560dfac31"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 20/20 [36:21<00:00, 109.06s/trial, best loss: -0.7466774058138965]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99    740838\n",
            "           1       0.10      0.16      0.12      6573\n",
            "\n",
            "    accuracy                           0.98    747411\n",
            "   macro avg       0.55      0.57      0.56    747411\n",
            "weighted avg       0.98      0.98      0.98    747411\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second Part\n",
        "## Calculating Margin\n",
        "\n",
        "This function calculates the financial impact (margin or loss) of a single transaction based on the actual and predicted labels. It incorporates a cost-sensitive approach by assigning different costs to false positives and false negatives, depending on the transaction amount. This allows the model to prioritize decisions that minimize financial losses while maximizing profits."
      ],
      "metadata": {
        "id": "tZQd1N-xPrjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_margin(actual, predicted, transaction_amount):\n",
        "    \"\"\"\n",
        "    Calculate the margin for a single transaction based on the cost matrix.\n",
        "\n",
        "    Parameters:\n",
        "        actual (int): Actual class (0 = non-fraudulent, 1 = fraudulent).\n",
        "        predicted (int): Predicted class (0 = accepted, 1 = rejected).\n",
        "        transaction_amount (float): Transaction amount.\n",
        "\n",
        "    Returns:\n",
        "        float: Margin generated or lost.\n",
        "    \"\"\"\n",
        "    # Define the margin rate for good (non-fraudulent) transactions\n",
        "    r = 0.05  # Margin rate for good transactions (e.g., 5% profit margin)\n",
        "\n",
        "    # Case 1: True Negative (TN)\n",
        "    # The transaction is non-fraudulent (actual == 0) and was correctly predicted as non-fraudulent (predicted == 0).\n",
        "    # In this case, we earn a margin proportional to the transaction amount.\n",
        "    if actual == 0 and predicted == 0:\n",
        "        return r * transaction_amount\n",
        "\n",
        "    # Case 2: False Positive (FP)\n",
        "    # The transaction is non-fraudulent (actual == 0) but was incorrectly predicted as fraudulent (predicted == 1).\n",
        "    # This results in a loss due to rejecting a valid transaction. The loss is proportional to the margin rate.\n",
        "    elif actual == 0 and predicted == 1:\n",
        "        return -0.7 * r * transaction_amount  # Loss is 70% of the potential margin\n",
        "\n",
        "    # Case 3: False Negative (FN)\n",
        "    # The transaction is fraudulent (actual == 1) but was incorrectly predicted as non-fraudulent (predicted == 0).\n",
        "    # The loss depends on the transaction amount, as larger fraudulent transactions result in greater losses.\n",
        "    elif actual == 1 and predicted == 0:\n",
        "        if transaction_amount <= 20:\n",
        "            return 0  # No loss for very small transactions (below €20)\n",
        "        elif transaction_amount <= 50:\n",
        "            return -0.2 * transaction_amount  # 20% loss for transactions up to €50\n",
        "        elif transaction_amount <= 100:\n",
        "            return -0.3 * transaction_amount  # 30% loss for transactions up to €100\n",
        "        elif transaction_amount <= 200:\n",
        "            return -0.5 * transaction_amount  # 50% loss for transactions up to €200\n",
        "        else:\n",
        "            return -0.8 * transaction_amount  # 80% loss for transactions above €200\n",
        "\n",
        "    # Case 4: True Positive (TP)\n",
        "    # The transaction is fraudulent (actual == 1) and was correctly predicted as fraudulent (predicted == 1).\n",
        "    # In this case, there is no gain or loss because the fraudulent transaction was successfully blocked.\n",
        "    elif actual == 1 and predicted == 1:\n",
        "        return 0  # No gain or loss"
      ],
      "metadata": {
        "id": "QRH3LUdGusFa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize total margin\n",
        "# This variable will accumulate the total margin across all transactions.\n",
        "total_margin = 0\n",
        "\n",
        "# Loop through all predictions\n",
        "# Iterate over each transaction in the test dataset to calculate the margin for each one.\n",
        "for i in range(len(y_test)):\n",
        "    # Retrieve the actual and predicted labels for the current transaction\n",
        "    # `y_test.iloc[i]` gives the true label (0 or 1) for the transaction.\n",
        "    # `y_pred[i]` gives the predicted label (0 or 1) from the model.\n",
        "    actual = y_test.iloc[i]\n",
        "    predicted = y_pred[i]\n",
        "\n",
        "    # Retrieve the transaction amount for the current transaction\n",
        "    # `X_test.iloc[i][\"Montant\"]` extracts the transaction amount (e.g., monetary value)\n",
        "    # associated with the current transaction.\n",
        "    transaction_amount = X_test.iloc[i][\"Montant\"]\n",
        "\n",
        "    # Calculate margin for the current transaction\n",
        "    # Call the `calculate_margin` function, which computes the margin based on:\n",
        "    # - The actual label (whether the transaction was fraudulent or not),\n",
        "    # - The predicted label (whether the model correctly identified it),\n",
        "    # - The transaction amount (to weigh the impact of the prediction).\n",
        "    total_margin += calculate_margin(actual, predicted, transaction_amount)\n",
        "\n",
        "# Print the total margin\n",
        "# After processing all transactions, display the accumulated total margin.\n",
        "print(f\"Total Margin: {total_margin}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDZAfe36uvLS",
        "outputId": "fc4c6c7b-e65d-44c1-f4e3-f87c4b908a8a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Margin: 1963703.8396503185\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict probabilities for the positive class\n",
        "# The `predict_proba` method of the XGBoost model returns a 2D array where:\n",
        "# - The first column corresponds to the probability of the negative class (class 0).\n",
        "# - The second column corresponds to the probability of the positive class (class 1).\n",
        "# By using `[:, 1]`, we extract only the probabilities for the positive class (class 1).\n",
        "y_pred_proba = best_model.predict_proba(X_test)[:, 1]"
      ],
      "metadata": {
        "id": "966Xu90LvTmz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define a range of thresholds between 0.1 and 0.9, with 50 evenly spaced values\n",
        "# This will be used to evaluate different decision thresholds for classification\n",
        "thresholds = np.linspace(0.1, 0.9, 50)\n",
        "\n",
        "# Initialize variables to track the best threshold and the corresponding margin\n",
        "# `best_threshold` stores the threshold that yields the highest total margin\n",
        "# `best_margin` is initialized to negative infinity to ensure any valid margin will replace it\n",
        "best_threshold = 0\n",
        "best_margin = float('-inf')\n",
        "\n",
        "# Evaluate each threshold in the defined range\n",
        "for threshold in thresholds:\n",
        "    # Convert predicted probabilities (`y_pred_proba`) into binary predictions (0 or 1)\n",
        "    # based on the current threshold. If the probability >= threshold, predict 1; otherwise, predict 0.\n",
        "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
        "\n",
        "    # Initialize a variable to calculate the total margin for the current threshold\n",
        "    total_margin = 0\n",
        "\n",
        "    # Iterate through each test sample to calculate the margin\n",
        "    for i in range(len(y_test)):\n",
        "        # Get the actual class label (0 or 1) for the current sample\n",
        "        actual = y_test.iloc[i]\n",
        "\n",
        "        # Get the predicted class label (0 or 1) for the current sample\n",
        "        predicted = y_pred[i]\n",
        "\n",
        "        # Get the transaction amount (e.g., monetary value) for the current sample\n",
        "        transaction_amount = X_test.iloc[i][\"Montant\"]\n",
        "\n",
        "        # Calculate the margin for the current sample using a custom function `calculate_margin`\n",
        "        # This function should take into account the actual label, predicted label, and transaction amount\n",
        "        total_margin += calculate_margin(actual, predicted, transaction_amount)\n",
        "\n",
        "    # Check if the total margin for the current threshold is better than the best margin so far\n",
        "    # If yes, update the best margin and the corresponding threshold\n",
        "    if total_margin > best_margin:\n",
        "        best_margin = total_margin\n",
        "        best_threshold = threshold\n",
        "\n",
        "# Print the best threshold and the corresponding total margin\n",
        "print(f\"Best Threshold: {best_threshold}\")\n",
        "print(f\"Best Total Margin: {best_margin}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jo1iSBRu1Hr",
        "outputId": "4bf7d746-d9a3-49f7-eaf7-717c05413caa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Threshold: 0.7040816326530612\n",
            "Best Total Margin: 2008401.9618003068\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing Margins for Fraud Detection Strategies A/B Testing"
      ],
      "metadata": {
        "id": "p9v-ztxSSCY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group A: No fraud detection (all transactions are accepted)\n",
        "group_a_margin = 0\n",
        "for i in range(len(y_test)):\n",
        "    actual = y_test.iloc[i]  # Actual label (0: Non-Fraud, 1: Fraud)\n",
        "    transaction_amount = X_test.iloc[i][\"Montant\"]  # Transaction amount for the current instance\n",
        "    group_a_margin += calculate_margin(actual, 0, transaction_amount)  # Always accept (predicted=0)\n",
        "\n",
        "# Explanation:\n",
        "# In this group, we simulate a scenario where no fraud detection is applied.\n",
        "# All transactions are accepted regardless of whether they are fraudulent or not.\n",
        "# The margin is calculated based on the actual label and the transaction amount.\n",
        "\n",
        "# Group B: Fraud detection applied\n",
        "group_b_margin = 0\n",
        "for i in range(len(y_test)):\n",
        "    actual = y_test.iloc[i]  # Actual label (0: Non-Fraud, 1: Fraud)\n",
        "    predicted = y_pred[i]  # Predicted label from the fraud detection model (0: Non-Fraud, 1: Fraud)\n",
        "    transaction_amount = X_test.iloc[i][\"Montant\"]  # Transaction amount for the current instance\n",
        "    group_b_margin += calculate_margin(actual, predicted, transaction_amount)\n",
        "\n",
        "# Explanation:\n",
        "# In this group, we apply the fraud detection model to predict whether a transaction is fraudulent.\n",
        "# The margin is calculated based on the actual label, the predicted label, and the transaction amount.\n",
        "# This simulates the impact of using a fraud detection system.\n",
        "\n",
        "# Print the results for comparison\n",
        "print(f\"Group A Margin (No Fraud Detection): {group_a_margin}\")\n",
        "print(f\"Group B Margin (With Fraud Detection): {group_b_margin}\")\n",
        "\n",
        "# Explanation:\n",
        "# The margins for both groups are compared to evaluate the financial impact of applying fraud detection.\n",
        "# A higher margin in Group B indicates that the fraud detection model improves profitability by reducing losses due to fraud."
      ],
      "metadata": {
        "id": "aRpWotoau4-C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b659bad-8d13-4108-d0b2-be9c65f505c4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Group A Margin (No Fraud Detection): 1941851.6945002985\n",
            "Group B Margin (With Fraud Detection): 1982455.2089003082\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simulating Transaction Fraud Rates and Calculating Margins"
      ],
      "metadata": {
        "id": "4i9tnIZrSetV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Define a function to calculate margin based on actual fraud, predicted fraud, and transaction amount\n",
        "def calculate_margin(actual, predicted, transaction_amount):\n",
        "    \"\"\"\n",
        "    Calculate the margin for a transaction based on actual fraud status, predicted fraud status,\n",
        "    and the transaction amount.\n",
        "\n",
        "    Parameters:\n",
        "        actual (int): Actual fraud status (1 for fraud, 0 for non-fraud).\n",
        "        predicted (int): Predicted fraud status (1 for fraud, 0 for non-fraud).\n",
        "        transaction_amount (float): The transaction amount.\n",
        "\n",
        "    Returns:\n",
        "        float: The calculated margin.\n",
        "    \"\"\"\n",
        "    # Example margin logic (customize as needed)\n",
        "    if actual == 1 and predicted == 1:  # Correctly predicted fraud\n",
        "        return -transaction_amount * 0.1  # Assume a 10% loss due to fraud prevention\n",
        "    elif actual == 1 and predicted == 0:  # Missed fraud\n",
        "        return -transaction_amount  # Full loss of transaction amount\n",
        "    elif actual == 0 and predicted == 1:  # False positive\n",
        "        return -transaction_amount * 0.05  # Assume a 5% cost for false positives\n",
        "    else:  # Correctly predicted non-fraud\n",
        "        return transaction_amount * 0.02  # Assume a 2% profit for successful transactions\n",
        "\n",
        "\n",
        "# Simulate transactions with varying fraud rates\n",
        "fraud_rates = [0.01, 0.05, 0.1, 0.2]  # Example fraud rates (1%, 5%, 10%, 20%)\n",
        "results = []  # Store results for each fraud rate simulation\n",
        "\n",
        "for fraud_rate in fraud_rates:\n",
        "    # Simulate test labels (y_test) based on the current fraud rate\n",
        "    simulated_y_test = [\n",
        "        1 if random.random() < fraud_rate else 0 for _ in range(len(y_test))\n",
        "    ]\n",
        "    simulated_margin = 0  # Initialize the total margin for this simulation\n",
        "\n",
        "    # Iterate through each transaction in the test set\n",
        "    for i in range(len(simulated_y_test)):\n",
        "        actual = simulated_y_test[i]  # Simulated actual fraud status\n",
        "        predicted = y_pred[i]  # Model's predicted fraud status\n",
        "        transaction_amount = X_test.iloc[i][\"Montant\"]  # Transaction amount from test data\n",
        "\n",
        "        # Calculate the margin for the current transaction and add it to the total margin\n",
        "        simulated_margin += calculate_margin(actual, predicted, transaction_amount)\n",
        "\n",
        "    # Store the results for the current fraud rate\n",
        "    results.append((fraud_rate, simulated_margin))\n",
        "\n",
        "# Print the results of the simulations\n",
        "print(\"Simulation Results:\")\n",
        "for fraud_rate, margin in results:\n",
        "    print(f\"Fraud Rate: {fraud_rate}, Simulated Margin: {margin}\")"
      ],
      "metadata": {
        "id": "zycvp5hOvEbq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0abc2cac-8c36-4ad7-e827-4867db6a9563"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fraud Rate: 0.01, Simulated Margin: 2109040.7784003373\n",
            "Fraud Rate: 0.05, Simulated Margin: 1274532.9787500459\n",
            "Fraud Rate: 0.1, Simulated Margin: 242417.2435499819\n",
            "Fraud Rate: 0.2, Simulated Margin: -1858523.518149816\n"
          ]
        }
      ]
    }
  ]
}