{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6rZI4XSOf8g"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd  # For data manipulation and analysis\n",
        "import xgboost as xgb  # For using the XGBoost algorithm\n",
        "from sklearn.metrics import classification_report  # For evaluating model performance\n",
        "\n",
        "# Load the preprocessed training and testing datasets from Parquet files\n",
        "# Parquet is a columnar storage file format optimized for use with big data processing frameworks\n",
        "train_df = pd.read_parquet(\"preprocessed_train_data.parquet\")  # Load the training dataset\n",
        "test_df = pd.read_parquet(\"preprocessed_test_data.parquet\")  # Load the testing dataset\n",
        "\n",
        "# Define features (independent variables) and target (dependent variable)\n",
        "# Features are all columns except the target column \"FlagImpaye\"\n",
        "X_train = train_df.drop(columns=[\"FlagImpaye\"])  # Features for the training set\n",
        "y_train = train_df[\"FlagImpaye\"].astype(int)  # Target for the training set, converted to numeric (int)\n",
        "\n",
        "X_test = test_df.drop(columns=[\"FlagImpaye\"])  # Features for the testing set\n",
        "y_test = test_df[\"FlagImpaye\"].astype(int)  # Target for the testing set, converted to numeric (int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhBoxMOshW-L"
      },
      "source": [
        "#1. Cost-Sensitive Learning with XGBoost\n",
        "XGBoost provides the scale_pos_weight parameter to handle imbalanced datasets. This parameter adjusts the weight of the positive class (fraudulent transactions) relative to the negative class (non-fraudulent transactions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkcvUWo9Pd0Q",
        "outputId": "55f7e21c-26b4-41aa-ef94-36f282fe2511"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.68      0.81    740838\n",
            "           1       0.02      0.67      0.04      6573\n",
            "\n",
            "    accuracy                           0.68    747411\n",
            "   macro avg       0.51      0.67      0.42    747411\n",
            "weighted avg       0.99      0.68      0.80    747411\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Calculate scale_pos_weight for cost-sensitive learning\n",
        "# The scale_pos_weight parameter is used to handle class imbalance in binary classification.\n",
        "# It adjusts the weight of the positive class (fraudulent transactions) relative to the negative class (non-fraudulent transactions).\n",
        "# This is calculated as the ratio of the number of negative samples to the number of positive samples.\n",
        "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
        "\n",
        "# Train XGBoost model with cost-sensitive learning\n",
        "# Initialize the XGBoost classifier with specific parameters:\n",
        "# - scale_pos_weight: Adjusts the balance between classes based on the calculated ratio.\n",
        "# - objective=\"binary:logistic\": Specifies that this is a binary classification problem with logistic loss.\n",
        "# - eval_metric=\"logloss\": Sets the evaluation metric to log loss, which is commonly used for binary classification.\n",
        "# - random_state=42: Ensures reproducibility by fixing the random seed.\n",
        "model_xgb = xgb.XGBClassifier(\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    objective=\"binary:logistic\",\n",
        "    eval_metric=\"logloss\",\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit the XGBoost model to the training data\n",
        "# This trains the model using the features (X_train) and target variable (y_train).\n",
        "model_xgb.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "# Use the trained model to make predictions on the test set (X_test).\n",
        "y_pred_xgb = model_xgb.predict(X_test)\n",
        "\n",
        "# Print a classification report to evaluate model performance\n",
        "# The classification report includes metrics such as precision, recall, F1-score, and support for each class.\n",
        "# These metrics help assess how well the model performs, especially for imbalanced datasets.\n",
        "print(classification_report(y_test, y_pred_xgb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go6fbPQlhMzZ"
      },
      "source": [
        "#2. Cost-Sensitive Learning with LightGBM\n",
        "\n",
        "LightGBM allows us to specify the is_unbalance parameter or provide instance weights directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07g1YYKnPfmY",
        "outputId": "11fd245f-76c7-4e16-df18-313f9626eadf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 23346, number of negative: 3865122\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.243284 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2107\n",
            "[LightGBM] [Info] Number of data points in the train set: 3888468, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006004 -> initscore=-5.109323\n",
            "[LightGBM] [Info] Start training from score -5.109323\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.60      0.75    740838\n",
            "           1       0.01      0.66      0.03      6573\n",
            "\n",
            "    accuracy                           0.60    747411\n",
            "   macro avg       0.50      0.63      0.39    747411\n",
            "weighted avg       0.99      0.60      0.74    747411\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "# Create LightGBM datasets\n",
        "# LightGBM uses its own Dataset format for training and evaluation.\n",
        "# `X_train` contains the feature matrix for the training set, and `y_train` contains the corresponding labels.\n",
        "train_data = lgb.Dataset(X_train, label=y_train)\n",
        "\n",
        "# Similarly, create a dataset for the test set. The `reference` parameter ensures that the test set aligns with the training set's structure.\n",
        "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
        "\n",
        "# Define parameters for cost-sensitive learning\n",
        "# These parameters configure the LightGBM model for binary classification with cost-sensitive adjustments.\n",
        "params = {\n",
        "    \"objective\": \"binary\",  # Specifies that this is a binary classification problem.\n",
        "    \"metric\": \"binary_logloss\",  # Use binary log loss as the evaluation metric.\n",
        "    \"is_unbalance\": True,  # Automatically adjusts for class imbalance by weighting classes inversely proportional to their frequency.\n",
        "    \"boosting_type\": \"gbdt\",  # Gradient Boosting Decision Tree (GBDT) is the default boosting method.\n",
        "    \"random_state\": 42  # Set a random seed for reproducibility.\n",
        "}\n",
        "\n",
        "# Train LightGBM model\n",
        "# The `train` function trains the model using the specified parameters and training dataset.\n",
        "model_lgb = lgb.train(params, train_data)\n",
        "\n",
        "# Evaluate the model\n",
        "# Predict probabilities for the test set. LightGBM outputs probabilities for the positive class by default.\n",
        "y_pred_lgb = model_lgb.predict(X_test)\n",
        "\n",
        "# Convert predicted probabilities into binary predictions (0 or 1).\n",
        "# A threshold of 0.5 is used: if the predicted probability is greater than 0.5, classify as 1; otherwise, classify as 0.\n",
        "y_pred_binary = [1 if x > 0.5 else 0 for x in y_pred_lgb]\n",
        "\n",
        "# Print a classification report to evaluate the model's performance.\n",
        "# This includes metrics like precision, recall, F1-score, and support for each class.\n",
        "print(classification_report(y_test, y_pred_binary))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPu6yN12g_-C"
      },
      "source": [
        "#Cost-Sensitive Learning with XGBoost + Bayesian Optimization\n",
        "Bayesian optimization can be used to fine-tune hyperparameters, including scale_pos_weight. We'll use the hyperopt library for this purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXsJRv4UPia6",
        "outputId": "9a85432a-5ac2-4766-bf02-538f1fd805d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [51:23<00:00, 61.67s/trial, best loss: -0.7475658617064199] \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99    740838\n",
            "           1       0.14      0.12      0.13      6573\n",
            "\n",
            "    accuracy                           0.99    747411\n",
            "   macro avg       0.57      0.56      0.56    747411\n",
            "weighted avg       0.98      0.99      0.99    747411\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from hyperopt import fmin, tpe, hp, Trials\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Define the objective function for Bayesian optimization\n",
        "def objective(params):\n",
        "    \"\"\"\n",
        "    This function defines the objective to minimize during Bayesian optimization.\n",
        "    It trains an XGBoost model using the given hyperparameters and evaluates its performance\n",
        "    on the test set using the ROC-AUC score. The negative AUC is returned because `fmin`\n",
        "    minimizes the objective function.\n",
        "    \"\"\"\n",
        "    # Create an XGBoost classifier with the current set of hyperparameters\n",
        "    model = xgb.XGBClassifier(\n",
        "        scale_pos_weight=params['scale_pos_weight'],  # Weight for positive class (cost-sensitive learning)\n",
        "        max_depth=int(params['max_depth']),          # Maximum depth of the tree\n",
        "        learning_rate=params['learning_rate'],       # Step size shrinkage used in updates\n",
        "        n_estimators=int(params['n_estimators']),    # Number of boosting rounds\n",
        "        random_state=42                              # Fixed random seed for reproducibility\n",
        "    )\n",
        "\n",
        "    # Train the model on the training data\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict probabilities for the positive class on the test set\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Calculate the ROC-AUC score and return its negative value (to minimize)\n",
        "    return -roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "# Define the search space for hyperparameters\n",
        "space = {\n",
        "    'scale_pos_weight': hp.uniform('scale_pos_weight', 1, 10),  # Uniform distribution for scale_pos_weight\n",
        "    'max_depth': hp.quniform('max_depth', 3, 10, 1),           # Quantized uniform distribution for max_depth\n",
        "    'learning_rate': hp.loguniform('learning_rate', -5, 0),   # Log-uniform distribution for learning_rate\n",
        "    'n_estimators': hp.quniform('n_estimators', 50, 500, 50)   # Quantized uniform distribution for n_estimators\n",
        "}\n",
        "\n",
        "# Perform Bayesian optimization\n",
        "trials = Trials()  # Object to store details of each evaluation\n",
        "best_params = fmin(\n",
        "    fn=objective,            # Objective function to minimize\n",
        "    space=space,             # Search space for hyperparameters\n",
        "    algo=tpe.suggest,        # Tree-structured Parzen Estimator (TPE) algorithm for optimization\n",
        "    max_evals=50,            # Maximum number of evaluations\n",
        "    trials=trials            # Store trial information\n",
        ")\n",
        "\n",
        "# Train the final model with the best parameters found by Bayesian optimization\n",
        "best_model = xgb.XGBClassifier(\n",
        "    scale_pos_weight=best_params['scale_pos_weight'],  # Use the best scale_pos_weight\n",
        "    max_depth=int(best_params['max_depth']),          # Convert max_depth to integer\n",
        "    learning_rate=best_params['learning_rate'],       # Use the best learning_rate\n",
        "    n_estimators=int(best_params['n_estimators']),    # Convert n_estimators to integer\n",
        "    random_state=42                                    # Fixed random seed for reproducibility\n",
        ")\n",
        "best_model.fit(X_train, y_train)  # Train the model on the full training set\n",
        "\n",
        "# Evaluate the final model on the test set\n",
        "y_pred = best_model.predict(X_test)  # Predict class labels for the test set\n",
        "print(classification_report(y_test, y_pred))  # Print a detailed classification report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifluR8lnWs6d"
      },
      "source": [
        "# Random Forest Model with Class Weighting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_dXVZH7Ws6f",
        "outputId": "cf3bb319-5659-469c-8ddf-d1eb7241f581"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00    740838\n",
            "           1       0.45      0.01      0.03      6573\n",
            "\n",
            "    accuracy                           0.99    747411\n",
            "   macro avg       0.72      0.51      0.51    747411\n",
            "weighted avg       0.99      0.99      0.99    747411\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "# Initialize the Random Forest Classifier with class weighting to handle imbalance\n",
        "# `class_weight=\"balanced\"` automatically adjusts weights inversely proportional to class frequencies\n",
        "best_rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,       # Number of trees in the forest\n",
        "    class_weight=\"balanced\",  # Adjusts weights to handle imbalanced classes\n",
        "    random_state=42         # Ensures reproducibility of results\n",
        ")\n",
        "\n",
        "# Train the Random Forest model on the training data\n",
        "best_rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the Random Forest model on the test data\n",
        "y_pred_rf = best_rf_model.predict(X_test)\n",
        "\n",
        "# Print the classification report to assess performance\n",
        "print(\"Random Forest Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_rf))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kc61E3GqWs6i"
      },
      "source": [
        "# Ensemble Model Creation and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QY2cvBCrWs6j",
        "outputId": "91a18146-2716-48b4-cf19-5233ddcf9e38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ensemble Model Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00    740838\n",
            "           1       0.32      0.04      0.08      6573\n",
            "\n",
            "    accuracy                           0.99    747411\n",
            "   macro avg       0.66      0.52      0.54    747411\n",
            "weighted avg       0.99      0.99      0.99    747411\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create an ensemble model using soft voting\n",
        "ensemble_model = VotingClassifier(\n",
        "    estimators=[\n",
        "        (\"xgb\", best_model),      # Optimized XGBoost model (previously tuned using Bayesian optimization)\n",
        "        (\"rf\", best_rf_model)     # Optimized Random Forest model (assumed to be tuned elsewhere in the code)\n",
        "    ],\n",
        "    voting=\"soft\"                 # Use predicted probabilities for voting (soft voting improves performance for imbalanced datasets)\n",
        ")\n",
        "\n",
        "# Train the ensemble model\n",
        "# The ensemble model combines predictions from both XGBoost and Random Forest models\n",
        "# Soft voting aggregates the predicted probabilities from each model to make the final prediction\n",
        "ensemble_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the ensemble model\n",
        "# Predict the target variable on the test set using the trained ensemble model\n",
        "y_pred_ensemble = ensemble_model.predict(X_test)\n",
        "\n",
        "# Print the classification report for the ensemble model\n",
        "# This includes metrics such as precision, recall, F1-score, and support for each class\n",
        "print(\"Ensemble Model Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_ensemble))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kTlwwZsWs6l"
      },
      "source": [
        "# Calculating Margin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwXrR-n5Ws6m"
      },
      "outputs": [],
      "source": [
        "def calculate_margin(actual, predicted, transaction_amount):\n",
        "    \"\"\"\n",
        "    Calculate the margin for a single transaction based on the cost matrix.\n",
        "\n",
        "    Parameters:\n",
        "        actual (int): Actual class (0 = non-fraudulent, 1 = fraudulent).\n",
        "        predicted (int): Predicted class (0 = accepted, 1 = rejected).\n",
        "        transaction_amount (float): Transaction amount.\n",
        "\n",
        "    Returns:\n",
        "        float: Margin generated or lost.\n",
        "    \"\"\"\n",
        "    # Define the margin rate for good (non-fraudulent) transactions\n",
        "    r = 0.05  # Margin rate for good transactions (e.g., 5% profit margin)\n",
        "\n",
        "    # Case 1: True Negative (TN)\n",
        "    # The transaction is non-fraudulent (actual == 0) and was correctly predicted as non-fraudulent (predicted == 0).\n",
        "    # In this case, we earn a margin proportional to the transaction amount.\n",
        "    if actual == 0 and predicted == 0:\n",
        "        return r * transaction_amount\n",
        "\n",
        "    # Case 2: False Positive (FP)\n",
        "    # The transaction is non-fraudulent (actual == 0) but was incorrectly predicted as fraudulent (predicted == 1).\n",
        "    # This results in a loss due to rejecting a valid transaction. The loss is proportional to the margin rate.\n",
        "    elif actual == 0 and predicted == 1:\n",
        "        return -0.7 * r * transaction_amount  # Loss is 70% of the potential margin\n",
        "\n",
        "    # Case 3: False Negative (FN)\n",
        "    # The transaction is fraudulent (actual == 1) but was incorrectly predicted as non-fraudulent (predicted == 0).\n",
        "    # The loss depends on the transaction amount, as larger fraudulent transactions result in greater losses.\n",
        "    elif actual == 1 and predicted == 0:\n",
        "        if transaction_amount <= 20:\n",
        "            return 0  # No loss for very small transactions (below €20)\n",
        "        elif transaction_amount <= 50:\n",
        "            return -0.2 * transaction_amount  # 20% loss for transactions up to €50\n",
        "        elif transaction_amount <= 100:\n",
        "            return -0.3 * transaction_amount  # 30% loss for transactions up to €100\n",
        "        elif transaction_amount <= 200:\n",
        "            return -0.5 * transaction_amount  # 50% loss for transactions up to €200\n",
        "        else:\n",
        "            return -0.8 * transaction_amount  # 80% loss for transactions above €200\n",
        "\n",
        "    # Case 4: True Positive (TP)\n",
        "    # The transaction is fraudulent (actual == 1) and was correctly predicted as fraudulent (predicted == 1).\n",
        "    # In this case, there is no gain or loss because the fraudulent transaction was successfully blocked.\n",
        "    elif actual == 1 and predicted == 1:\n",
        "        return 0  # No gain or loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZUFckISWs6o"
      },
      "outputs": [],
      "source": [
        "# Predict probabilities for the positive class\n",
        "# The `predict_proba` method of the Ensemble model returns a 2D array where:\n",
        "# - The first column corresponds to the probability of the negative class (class 0).\n",
        "# - The second column corresponds to the probability of the positive class (class 1).\n",
        "# By using `[:, 1]`, we extract only the probabilities for the positive class (class 1).\n",
        "y_pred_proba = ensemble_model.predict_proba(X_test)[:, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcafciEPWs6r"
      },
      "source": [
        "## Margin Calculation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QX4gTRCOWs6p",
        "outputId": "3ae5eb5d-83cf-4f2f-f54a-b6edfb33c072"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Threshold: 0.37755102040816324\n",
            "Best Total Margin: 2015681.1537503034\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define a range of thresholds between 0.1 and 0.9, with 50 evenly spaced values\n",
        "# This will be used to evaluate different decision thresholds for classification\n",
        "thresholds = np.linspace(0.1, 0.9, 50)\n",
        "\n",
        "# Initialize variables to track the best threshold and the corresponding margin\n",
        "# `best_threshold` stores the threshold that yields the highest total margin\n",
        "# `best_margin` is initialized to negative infinity to ensure any valid margin will replace it\n",
        "best_threshold = 0\n",
        "best_margin = float('-inf')\n",
        "\n",
        "# Evaluate each threshold in the defined range\n",
        "for threshold in thresholds:\n",
        "    # Convert predicted probabilities (`y_pred_proba`) into binary predictions (0 or 1)\n",
        "    # based on the current threshold. If the probability >= threshold, predict 1; otherwise, predict 0.\n",
        "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
        "\n",
        "    # Initialize a variable to calculate the total margin for the current threshold\n",
        "    total_margin = 0\n",
        "\n",
        "    # Iterate through each test sample to calculate the margin\n",
        "    for i in range(len(y_test)):\n",
        "        # Get the actual class label (0 or 1) for the current sample\n",
        "        actual = y_test.iloc[i]\n",
        "\n",
        "        # Get the predicted class label (0 or 1) for the current sample\n",
        "        predicted = y_pred[i]\n",
        "\n",
        "        # Get the transaction amount (e.g., monetary value) for the current sample\n",
        "        transaction_amount = X_test.iloc[i][\"Montant\"]\n",
        "\n",
        "        # Calculate the margin for the current sample using a custom function `calculate_margin`\n",
        "        # This function should take into account the actual label, predicted label, and transaction amount\n",
        "        total_margin += calculate_margin(actual, predicted, transaction_amount)\n",
        "\n",
        "    # Check if the total margin for the current threshold is better than the best margin so far\n",
        "    # If yes, update the best margin and the corresponding threshold\n",
        "    if total_margin > best_margin:\n",
        "        best_margin = total_margin\n",
        "        best_threshold = threshold\n",
        "\n",
        "# Print the best threshold and the corresponding total margin\n",
        "print(f\"Best Threshold: {best_threshold}\")\n",
        "print(f\"Best Total Margin: {best_margin}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "fraud_test",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
